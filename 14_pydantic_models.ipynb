{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IJvl-EEESCA"
      },
      "source": [
        "# <font color=red>LangChain:  Using the PydanticOutputParser </br>to derive Pydantic Models from the output of LLMs</font>\n",
        "- https://docs.langchain.com/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>\n",
        "We use the PydanticOutputParser to parse output from the LLM into a Pydantic Model.</br>\n",
        "We ask the LLM to produce output conforming to a JSON schema for the Pydantic Model.</br>  \n",
        "This gives us a nice structured result providing a comfortable programming model.\n",
        "</h4>\n",
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif; font-size:18px;\"><font color=orange>\n",
        "## Demo 1 - Use an OpenAI gpt-x model withOUT a chain, just directly using the model\n",
        "</font></span></br></br>\n",
        "The OpenAI models are good at producing output that conforms to a provided JSON schema.</br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, time, json\n",
        "\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "class Info(BaseModel):\n",
        "    first_name:  str = Field(description=\"first name of the person\")\n",
        "    last_name:   str = Field(description=\"last name  of the person\")\n",
        "    age_died:    int = Field(description=\"age of the perso when they died\")\n",
        "    spouse_name: str = Field(description=\"name of the person's spouse\")\n",
        "    dog_name:    str = Field(description=\"name of the person's dog\")     # often not found\n",
        "    widget_name: str = Field(description=\"name of the person's widget\")  # None\n",
        "\n",
        "    @validator(\"age_died\")  # just for demo validation\n",
        "    @classmethod            # better to have this in v2\n",
        "    def check_age_died(cls, age):\n",
        "        if age < 1 or age > 101:\n",
        "            raise ValueError(\"probable wrong value for age_died \")\n",
        "        return age\n",
        "\n",
        "\n",
        "template_string = \"\"\"\n",
        "You are a helpful assistant providing information about American history.\n",
        "Use the formatting instructions below to provide the answers to user queries.\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "FORMATTING_INSTRUCTIONS:\n",
        "{format_instructions}\n",
        "If you can not find a value for a field, then assign it the value \"None\".\n",
        "\"\"\"\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0.0)\n",
        "print(\"MODELNAME\",llm.model_name)\n",
        "\n",
        "pydantic_parser = PydanticOutputParser(pydantic_object=Info)\n",
        "format_instructions = pydantic_parser.get_format_instructions()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=template_string)\n",
        "\n",
        "query = \"How old was the first president of the USA when he died?\"\n",
        "messages = prompt.format_messages(query=query, format_instructions=format_instructions)\n",
        "\n",
        "print(\"MSGS\",messages)\n",
        "\n",
        "output = llm(messages)\n",
        "\n",
        "print(\"OUTCONTENT\",output.content)\n",
        "\n",
        "info1 = pydantic_parser.parse(output.content)\n",
        "\n",
        "print(\"info1TYPE\",type(info1))\n",
        "print(\"info1CONTENT\",info1)\n",
        "print(\"info1\",info1.first_name,info1.last_name,info1.age_died)\n",
        "\n",
        "#### LangChain provides:\n",
        "####     \"\"\"## Output FixingParser\"\"\"\n",
        "####     \"\"\"## RetryOutputParser\n",
        "#### But I have not needed them using OpenAI.\n",
        "#### I used them with some success in the Mistral demo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif; font-size:18px;\"><font color=orange>\n",
        "## Demo 2 - Use an OpenAI gpt-x model WITH a chain\n",
        "</font></span></br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, time, json\n",
        "\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "class Info(BaseModel):\n",
        "    first_name:  str = Field(description=\"first name of the person\")\n",
        "    last_name:   str = Field(description=\"last name  of the person\")\n",
        "    age_died:    int = Field(description=\"age of the perso when they died\")\n",
        "    spouse_name: str = Field(description=\"name of the person's spouse\")\n",
        "    dog_name:    str = Field(description=\"name of the person's dog\")     # often not found\n",
        "    widget_name: str = Field(description=\"name of the person's widget\")  # None\n",
        "\n",
        "    @validator(\"age_died\")  # just for demo validation\n",
        "    @classmethod            # better to have this in v2\n",
        "    def check_age_died(cls, age):\n",
        "        if age < 1 or age > 101:\n",
        "            raise ValueError(\"probable wrong value for age_died \")\n",
        "        return age\n",
        "\n",
        "\n",
        "template_string = \"\"\"\n",
        "You are a helpful assistant providing information about American history.\n",
        "Use the formatting instructions below to provide the answers to user queries.\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "FORMATTING_INSTRUCTIONS:\n",
        "{format_instructions}\n",
        "If you can not find a value for a field, then assign it the value \"None\".\n",
        "\"\"\"\n",
        "\n",
        "pydantic_parser = PydanticOutputParser(pydantic_object=Info)\n",
        "format_instructions = pydantic_parser.get_format_instructions()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=template_string,)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0.0)\n",
        "print(\"MODELNAME\",llm.model_name)\n",
        "chain = LLMChain(llm=llm, prompt=prompt, output_parser=pydantic_parser)\n",
        "\n",
        "query = \"How old was the first president of the USA when he died?\"\n",
        "\n",
        "info1 = chain.predict(query=query, format_instructions=format_instructions)\n",
        "print(\"DBG\",type(info1))\n",
        "\n",
        "print(\"info1TYPE\",type(info1))\n",
        "print(\"info1CONTENT\",info1)\n",
        "print(\"info1\",info1.first_name,info1.last_name,info1.age_died)\n",
        "\n",
        "#### LangChain provides:\n",
        "####     \"\"\"## Output FixingParser\"\"\"\n",
        "####     \"\"\"## RetryOutputParser\n",
        "#### But I have not needed them using OpenAI.\n",
        "#### I used them with some success in the Mistral demo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif; font-size:18px;\"><font color=orange>\n",
        "## Demo 3 - Use the Mistral model\n",
        "</font></span></br>\n",
        "The Mistral and Zephyr models were derived from Llama-2.</br>\n",
        "They are not quite as good as the OpenAI models at following a JSON schema \n",
        "to produce output which can be parsed into a Pydantic Model.  </br>\n",
        "But, they do an adequate job, and a little re-try logic typically gets the job done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, time, json\n",
        "\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "class Info(BaseModel):\n",
        "    first_name:  str = Field(description=\"first name of the person\")\n",
        "    last_name:   str = Field(description=\"last name  of the person\")\n",
        "    age_died:    int = Field(description=\"age of the perso when they died\")\n",
        "    spouse_name: str = Field(description=\"name of the person's spouse\")\n",
        "    dog_name:    str = Field(description=\"name of the person's dog\")     # often not found\n",
        "    widget_name: str = Field(description=\"name of the person's widget\")  # None\n",
        "\n",
        "    @validator(\"age_died\")  # demo validation\n",
        "    @classmethod            # better to have this in v2\n",
        "    def check_age_died(cls, age):\n",
        "        if age < 1 or age > 101:\n",
        "            raise ValueError(\"probable wrong value for age_died \")\n",
        "        return age\n",
        "\n",
        "## this top part is setting up to use Mistral with LangChain instead of gpt-x ########\n",
        "\n",
        "model_id = \"./Mistral-7B-Instruct-v0.1\"   # first, setup the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, device_map='auto', # torch_dtype=torch.float16,\n",
        ")\n",
        "model.eval()\n",
        "print(\"MODELID\",model_id)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)  # second, setup the tokenizer\n",
        "\n",
        "pipe = pipeline(            # third, setup the pipeline using the model and tokenizer\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.1,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)   # fourth / last, setup the LLM\n",
        "\n",
        "## conclude setting up to use Mistral with LangChain instead of gpt-x ########\n",
        "\n",
        "template_string = \"\"\"\n",
        "You are a helpful assistant providing information about American history.\n",
        "Use the formatting instructions below to provide the answers to user queries.\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "FORMATTING_INSTRUCTIONS:\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "# sometimes uses '' for un-found values; did not help to have this line:\n",
        "#    If you can not find a value for a field, then assign it the value \"None\".\n",
        "\n",
        "pydantic_parser = PydanticOutputParser(pydantic_object=Info)\n",
        "format_instructions = pydantic_parser.get_format_instructions()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=template_string,)\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt, output_parser=pydantic_parser)\n",
        "\n",
        "query = \"How old was the first president of the USA when he died?\"\n",
        "\n",
        "try:  # this code usually works fine\n",
        "    info1 = chain.predict(query=query, format_instructions=format_instructions)\n",
        "    print(\"info1CONTENT\",info1)\n",
        "    print(\"info1TYPE\",type(info1))\n",
        "except:  # this code has sometimes come into play, and it has worked fine\n",
        "    print(\"\\n**** TRYING FIX\\n\")\n",
        "    from langchain.output_parsers import OutputFixingParser\n",
        "    fix_parser = OutputFixingParser.from_llm(parser=pydantic_parser, llm=llm)\n",
        "    fix_chain = LLMChain(llm=llm, prompt=prompt, output_parser=fix_parser)\n",
        "    try:  # this code has run before with success at getting an answer\n",
        "        info1 = fix_chain.predict(query=query, format_instructions=format_instructions)\n",
        "        print(\"info1CONTENT\",info1)\n",
        "        print(\"info1TYPE\",type(info1))\n",
        "    except:  # this code has not successfully tested to get an answer\n",
        "        print(\"\\n**** DOING RETRY\\n\")\n",
        "        from langchain.output_parsers import RetryWithErrorOutputParser\n",
        "        retry_parser = RetryWithErrorOutputParser.from_llm(parser=pydantic_parser, llm=llm)\n",
        "        retry_chain = LLMChain(llm=llm, prompt=prompt, output_parser=retry_parser)\n",
        "        try:  # this code has not yet run so I am not sure it works\n",
        "            info1 = retry_chain.predict(query=query, format_instructions=format_instructions)\n",
        "            print(\"info1CONTENT\",info1)\n",
        "            print(\"info1TYPE\",type(info1))\n",
        "        except:\n",
        "            print(\"**** UNABLE to handle query:\",query)\n",
        "            exit(-1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
