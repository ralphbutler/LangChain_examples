{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IJvl-EEESCA"
      },
      "source": [
        "# <font color=red>LangChain:  Vector DBs</font>\n",
        "- https://docs.langchain.com/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Does LangChain Provide?\n",
        "+ Models\n",
        "  + embedding\n",
        "  + LLM (e.g. OpenAI)\n",
        "+ Prompts\n",
        "  + prompt templates\n",
        "  + few-shot\n",
        "  + example-selectors\n",
        "  + output parsers\n",
        "+ Chains (a multi-step workflow composed of <em>links</em>)</br>\n",
        "  + Links (one of: prompt, model, another chain)\n",
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "+ Vector Database Access\n",
        "  + Document Loaders\n",
        "  + Text Splitting \n",
        "</font></span>\n",
        "+ Memories (to facilitate chatbots or other 'iterative' sorts of apps)\n",
        "+ Agents (loop over Thought, Act, Observe)\n",
        "  + Tools\n",
        "    + math\n",
        "    + web search\n",
        "    + custom (user-defined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "## Vector Database Access\n",
        "</font></span>\n",
        "There are a lot of vector DBs available via LangChain.</br>\n",
        "Perhaps the most well-known commercial one is Pinecone.  It requires setup at their site.</br>\n",
        "We will be using a couple of free ones here:  FAISS and Chroma\n",
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "### Document Loaders and Text Splitting\n",
        "</font></span>\n",
        "These examples are somewhat longer because they not only demo using vector DBs, but also use chains to demo</br>\n",
        "the VDBs being used to retrieve useful info. \n",
        "<font color=green>These examples read a set of *.txt and *.pdf files from sub-directories named txt and pdf,</br>\n",
        "which are supplied with this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Demo loading txt files (no pdfs)\n",
        "This demo uses the <font color=green>Chroma</font> vector DB which is quite popular.</br> \n",
        "We also save (persist) the DB to disk.</br>\n",
        "We use a RetrievalQA chain to prove we can use the DB to retrieve document content and information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install chromadb   ## you may have to do this if not already installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma   ## use chroma vector DB\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "loader = DirectoryLoader('./', glob=\"./txt/*.txt\", loader_cls=TextLoader)\n",
        "# loader = TextLoader('./one_file.txt')\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=\"chroma_db\")\n",
        "\n",
        "## first, prove we can obtain the relevant docs\n",
        "retriever = vectordb.as_retriever()\n",
        "docs = retriever.get_relevant_documents( \"What did Abraham Lincoln say our fathers had brought forth on this continent?\" )\n",
        "print(\"RELEVANT DOCS\")\n",
        "print(docs)\n",
        "\n",
        "# k docs to return, default 4\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
        "print(\"SEARCH TYPE\",retriever.search_type)\n",
        "print(\"SEARCH KWARGS\",retriever.search_kwargs)\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0, model_name='gpt-4')\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "# get the sources from the response\n",
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])\n",
        "\n",
        "# full example\n",
        "query = \"What did Abraham Lincoln say our fathers had brought forth on this continent?\"\n",
        "llm_response = qa_chain(query)\n",
        "print(\"LLM_RESPONSE\")\n",
        "print(llm_response)\n",
        "print(\"PROCESSED OUTPUT\")\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Here is a simpler example with Chroma but using more high-level operations from LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "\n",
        "to_summarize = \"./txt/cuban.txt\"\n",
        "\n",
        "loaders = []\n",
        "for fn in os.listdir(\"./txt\"):\n",
        "    filename = \"./txt/\" + fn\n",
        "    loader = TextLoader(filename)\n",
        "    loaders.append(loader)\n",
        "index = VectorstoreIndexCreator().from_loaders(loaders)\n",
        "\n",
        "print(\"INDEX_VECTORSTORE\",index.vectorstore)\n",
        "print(\"AS_RETRIEVER\",index.vectorstore.as_retriever())\n",
        "print()\n",
        "\n",
        "query = \"What did Lincoln say that our fathers had brought forth on this continent?\"\n",
        "result = index.query(query)\n",
        "print(result,\"\\n\")\n",
        "\n",
        "query = \"What did Lincoln say that our fathers had brought forth on this continent?\"\n",
        "result = index.query_with_sources(query)\n",
        "# print(result,\"\\n\")\n",
        "print(result[\"answer\"])\n",
        "print(result[\"sources\"],\"\\n\")\n",
        "\n",
        "query = \"What happened on December 7, 1941?\"  # Dec. is abbreviated in the doc\n",
        "result = index.query_with_sources(query)\n",
        "# print(result,\"\\n\")\n",
        "print(result[\"answer\"])\n",
        "print(result[\"sources\"],\"\\n\")\n",
        "\n",
        "result = index.query(\"Summarize the general content of this document.\",\n",
        "                     retriever_kwargs={\"search_kwargs\": {\"filter\": {\"source\": to_summarize}}})\n",
        "print(result,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Demo loading pdf files (not txt)\n",
        "This demo uses the <font color=green>FAISS</font> vector DB.</br> \n",
        "We use a regular LLMChain chain to prove we can use the DB to retrieve document content and information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, openai, textwrap\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "\n",
        "pdf_filenames = [\"./pdf/\"+f for f in os.listdir('./pdf') if f.endswith(\".pdf\")]\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "all_pages = []\n",
        "for pdf_filename in pdf_filenames:\n",
        "    loader = PyPDFLoader(pdf_filename)\n",
        "    pages = loader.load_and_split()\n",
        "    all_pages.extend(pages)\n",
        "faiss_index = FAISS.from_documents(all_pages, embeddings)\n",
        "\n",
        "query = \"What is a generative agent?\"  ## NOTE: voyager_gpt4 pdf paper answers this\n",
        "\n",
        "# gpt-4 can handle up to 8192 tokens.  Set chunksize to 1000 and k to 8.\n",
        "docs = faiss_index.similarity_search(query, k=8)  # k=8 is default\n",
        "docs_page_content = \" \".join([d.page_content for d in docs])\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0.1)\n",
        "\n",
        "system_msg_template = \"\"\"\n",
        "    You are a helpful assistant that that can answer questions about content\n",
        "    obtained from pdf documents: {docs}\n",
        "    Only use the factual information from the documents to answer the question.\n",
        "    If you don't have enough information to answer the question, simply say \"I don't know\".\n",
        "    Your answer should be concise but provide sufficient detail to fully answer.\n",
        "\"\"\"\n",
        "system_msg_prompt = SystemMessagePromptTemplate.from_template(system_msg_template)\n",
        "\n",
        "###### NOTE: the human_template determines whether you ask which document is relevant to the\n",
        "######       question, or if you ask for the actual answer to the question\n",
        "human_template = \"Which document provides the best answer to this question: {question}\"\n",
        "human_template = \"Answer the following question: {question}\"\n",
        "human_msg_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages( [system_msg_prompt,human_msg_prompt])\n",
        "\n",
        "chain = LLMChain(llm=chat,prompt=chat_prompt)\n",
        "response = chain.run(question=query, docs=docs_page_content,return_source_documents=True)\n",
        "response = response.replace(\"\\n\", \"\")\n",
        "print(f\"\\nanswer:\\n    {textwrap.fill(response, width=70)}\")\n",
        "\n",
        "print(\"\\nsource page info:\")\n",
        "for doc in docs:\n",
        "    print(f\"    {doc.metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pinecone example code \n",
        "This code wil <font color=red>probably will not run for you</font>.  It depends on having a Pinecone account\n",
        "setup and usable via their API.</br>\n",
        "If you do have an account, then you should be able to modify the code to work. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### this program does not query openai it merely gets embeddings from openai, and \n",
        "#### places them into a pinecone vector DB along with the passage for each embedding\n",
        "\n",
        "import os, re, time\n",
        "\n",
        "content = \"\"\n",
        "with open(\"txt/gettysburg.txt\") as f:\n",
        "    content = \"\"\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        content += line.strip() + \" \"\n",
        "\n",
        "# --------\n",
        "\n",
        "import openai\n",
        "import pinecone  # pip install pinecone-client\n",
        "import pinecone.info\n",
        "import numpy as np\n",
        "\n",
        "EMBED_MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "# openai.organization = os.getenv(\"OPENAI_ORG\")\n",
        "with open(\"openaiorg.txt\") as f:\n",
        "    openai.organization = f.read().strip()\n",
        "\n",
        "# pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "with open(\"pineconekey.txt\") as f:\n",
        "    pinecone_api_key = f.read().strip()\n",
        "pinecone.init(api_key=pinecone_api_key, environment='us-west1-gcp')\n",
        "version_info = pinecone.info.version()\n",
        "server_version = \".\".join(version_info.server.split(\".\")[:2])\n",
        "client_version = \".\".join(version_info.client.split(\".\")[:2])\n",
        "print(server_version,client_version)\n",
        "## assert client_version == server_version, \"Please upgrade pinecone-client.\"\n",
        "\n",
        "passages = []\n",
        "num_words_per_chunk = 100\n",
        "words = content.split()\n",
        "for i in range(0, len(words), num_words_per_chunk):\n",
        "    chunk = \" \".join(words[i:i+num_words_per_chunk])\n",
        "    passages.append(chunk)\n",
        "\n",
        "batch_size = 32\n",
        "embeddings_all = []\n",
        "embeds_as_arrays = []  # need list of arrays to create index\n",
        "print(\"NUM_PASSAGES\",len(passages),\"APPROX_NUM_BATCHES\",len(passages)//batch_size)\n",
        "for i in range(0, len(passages), batch_size):\n",
        "    batch = passages[i : i+batch_size]\n",
        "    res = openai.Embedding.create(input=batch, engine=EMBED_MODEL)\n",
        "    embeds = [record['embedding'] for record in res['data']]\n",
        "    embeddings_all.extend(embeds)\n",
        "print(len(passages),len(embeddings_all))\n",
        "\n",
        "dim = len(embeddings_all[0])\n",
        "index_name = \"gettysburg\"\n",
        "if index_name in pinecone.list_indexes():\n",
        "    print(\"DEL INDEX\")\n",
        "    pinecone.delete_index(index_name)\n",
        "    print(\"DEL DONE\")\n",
        "    # pass\n",
        "ctime = time.time()\n",
        "print(\"CREATE INDEX\")\n",
        "pinecone.create_index(name=index_name, dimension=dim, metric=\"cosine\")\n",
        "print(\"CREATE DONE\",time.time()-ctime)\n",
        "index = pinecone.Index(index_name=index_name)\n",
        "vecIDs = [ str(i) for i in range(len(embeddings_all)) ]   # ids should be str\n",
        "meta = [{'text': passage} for passage in passages]\n",
        "rc = index.upsert(vectors=zip(vecIDs, embeddings_all, meta))\n",
        "print(rc)\n",
        "print( index.describe_index_stats() )\n",
        "len_embeds = len(embeddings_all[0])\n",
        "\n",
        "stime = time.time()\n",
        "\n",
        "query = \"Which speech began with 'Four score and seven years ago'?\"\n",
        "res = openai.Embedding.create (\n",
        "    input=[query], engine=EMBED_MODEL\n",
        ")\n",
        "q_embed = res['data'][0]['embedding']\n",
        "###### q_embed = np.array(q_embed).reshape( (1,len(q_embed)) )\n",
        "###### print(\"QEMBED\",q_embed.shape)\n",
        "\n",
        "rc = index.query(\n",
        "        vector=q_embed,\n",
        "        top_k=1,  # just going for 1 in this tiny demo\n",
        "        include_metadata=True,\n",
        "        include_values=True)\n",
        "for x in rc[\"matches\"]:\n",
        "    print(x[\"id\"])\n",
        "    print(x[\"metadata\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
