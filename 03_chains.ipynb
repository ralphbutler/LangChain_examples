{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IJvl-EEESCA"
      },
      "source": [
        "# <font color=red>LangChain:  Chains</font>\n",
        "- https://docs.langchain.com/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Does LangChain Provide?\n",
        "+ Models\n",
        "  + embedding\n",
        "  + LLM (e.g. OpenAI)\n",
        "+ Prompts\n",
        "  + prompt templates\n",
        "  + few-shot\n",
        "  + example-selectors\n",
        "  + output parsers\n",
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "+ Chains (a multi-step workflow composed of <em>links</em>)</br>\n",
        "  + Links (one of: prompt, model, another chain)\n",
        "</font></span>\n",
        "+ Vector Database Access\n",
        "  + Document Loaders\n",
        "  + Text Splitting \n",
        "+ Memories (to facilitate chatbots or other 'iterative' sorts of apps)\n",
        "+ Agents (loop over Thought, Act, Observe)\n",
        "  + Tools\n",
        "    + math\n",
        "    + web search\n",
        "    + custom (user-defined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "## Chains\n",
        "</font></span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A chain is a multi-step workflow, composed of links.\n",
        "A link is one of:\n",
        "- a prompt\n",
        "- an LLM\n",
        "- another chain\n",
        "#### Our first chain will be a repeat of the one we had in the Quickstart Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains  import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI   # switching to a chat model\n",
        "\n",
        "# note prompt's similarity to python f-strings\n",
        "template = \"Tell me a {joke_type} joke about {topic}.\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# here we are only printing the prompt, not using it with an LLM\n",
        "# we just want to show that we can make substitutions for the input_variables\n",
        "print(prompt.format(joke_type=\"dad\",topic=\"dogs\"))\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7, max_tokens=128)\n",
        "\n",
        "# now, let's create a chain consisting of the prompt and the LLM\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "response = llm_chain.run(joke_type=\"dad\",topic=\"my cat named Gizmo\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Next, a SimpleSequentialChain combining two chains, the second critiquing output of the first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains  import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import SimpleSequentialChain  # only pass a string between chains\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7, max_tokens=128,)\n",
        "\n",
        "template = \"What is a good Dad joke about {topic}?\"\n",
        "joke_prompt = PromptTemplate.from_template(template)\n",
        "joke_chain = LLMChain(prompt=joke_prompt, llm=llm)\n",
        "\n",
        "template = \"Is this a good joke: {the_joke}?\"\n",
        "critic_prompt = PromptTemplate.from_template(template)\n",
        "critic_chain = LLMChain(prompt=critic_prompt, llm=llm)\n",
        "\n",
        "# chain where we run the two chains in sequence\n",
        "seq_chain = SimpleSequentialChain(chains=[joke_chain, critic_chain], verbose=True)\n",
        "critique = seq_chain.run(\"my cat named Gizmo\")\n",
        "print(critique)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color=green>LLMMathChain </font>demo to show an example of a specialized built-in chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain import LLMMathChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "math_chain = LLMMathChain.from_llm(llm, verbose=True)\n",
        "\n",
        "response = math_chain.run(\"What is 13 raised to the .3432 power?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### We will repeat our first chain here but use LangChain's new Expression Language\n",
        "The Expression Language permits you to connect links with a pipe symbol, much like in a shell.</br>\n",
        "It is not totally mature yet, and I still prefer the functional method, but we do it here for completeness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains  import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI   # switching to a chat model\n",
        "from langchain.schema.output_parser import StrOutputParser   # output parser\n",
        "\n",
        "template = \"Tell me a {joke_type} joke about {topic}.\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "print(prompt.format(joke_type=\"dad\",topic=\"dogs\"))\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7, max_tokens=128)\n",
        "\n",
        "llm_chain = prompt | llm | StrOutputParser()    ## <-- the new language syntax\n",
        "\n",
        "## invoke INSTEAD of run and dictionary of arguments\n",
        "response = llm_chain.invoke( {\"joke_type\": \"dad\", \"topic\": \"my cat named Gizmo\"} )\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
