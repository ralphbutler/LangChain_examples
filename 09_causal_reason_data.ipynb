{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IJvl-EEESCA"
      },
      "source": [
        "# <font color=red>LangChain:  Example Generate Causal Reasoning Pre-training Data</font>\n",
        "- https://docs.langchain.com/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "## Demo 1 - Tiny example to generate data for a {subject}, e.g. biology\n",
        "</font></span>\n",
        "This program demos a way to generate causal reasoning examples that might be used to pre-train a model like AuroraGPT.</br>\n",
        "The program works with GPT-4 trained \"as-is\", i.e. it does not use \"trusted documents\" to make sure all is correct.</br>\n",
        "It also does not try to avoid hallucination with prompt directives such as \"don't make things up\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, random\n",
        "\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "NUM_EXAMPLES_TO_GEN = 11\n",
        "\n",
        "# MODEL = \"gpt-3.5-turbo\"\n",
        "MODEL = \"gpt-4\"\n",
        "\n",
        "TEMPLATE = \"\"\"\n",
        "You are generating data which will be used to fine-tune a Large Language Model.\n",
        "The model will be used to work with advanced high-school students studying {subject}.\n",
        "You will be generating single prompt/response pairs which present examples of\n",
        "causal reasoning that advanced high-school students in {subject} can understand.\n",
        "There may also be some previously generated examples provided in the prompt to\n",
        "help you to ensure uniqueness and diversity.\n",
        "Please keep each response to a \"reasonable\" length, i.e. no more than 100 words.\n",
        "The prompt/response pair that you create should be in this format:\n",
        "Prompt: <prompt goes here>\n",
        "Response: <response goes here>\n",
        "\\n\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "QUESTION = \"\"\"\n",
        "Please generate a single prompt/response pair which presents an example of\n",
        "causal reasoning that advanced high-school students in {subject} can understand.\n",
        "Be sure to avoid duplication with prior questions listed.\n",
        "Be sure to develop the response using a step-by-step process.\n",
        "\"\"\"\n",
        "\n",
        "llm = ChatOpenAI(model_name=MODEL,temperature=0.5) # ,max_tokens=100)\n",
        "prompt_template = PromptTemplate.from_template(TEMPLATE)\n",
        "answer_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "def generate_one_example(prev_examples, temperature=0.5):\n",
        "    if len(prev_examples) > 8:\n",
        "        prev_examples = random.sample(prev_examples,8)\n",
        "    prevs = \"\"\n",
        "    for example in prev_examples:\n",
        "        prevs += \"Previous example:\\n\" + example + \"\\n\"\n",
        "    question = prevs + QUESTION\n",
        "    answer = answer_chain.run(subject=\"biology\",question=question)\n",
        "    return answer\n",
        "\n",
        "prev_examples = []\n",
        "for i in range(NUM_EXAMPLES_TO_GEN):\n",
        "    print(f'Generating example {i}')\n",
        "    example = generate_one_example( prev_examples, temperature=0.5 )\n",
        "    prev_examples.append(example)\n",
        "\n",
        "for (i,pe) in enumerate(prev_examples):\n",
        "    print(f\"\\nCausal Reasoning Example {i}\\n----------------------------\")\n",
        "    print(f\"{pe}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
